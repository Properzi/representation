\chapter{}

\topic{Artin--Wedderburn theorem}

We first review the basic definitions concerning
finite-dimensional semisimple algebras. 
Proofs can be found in the
notes to the course \emph{Associative Algebras}, see
lectures 1, 2 and 3. 

Our base field will be the field $\C$ of complex numbers. 

\index{Algebra}
\index{Algebra!unitary}
A (complex) \textbf{algebra} $A$ is a is a (complex) vector space  
with an associative multiplication $A\times A\to A$ such that
\[
a(\lambda b+\mu c)=\lambda(ab)+\mu(ac),
\quad
(\lambda a+\mu b)c=\lambda(ac)+\mu (bc)
\]
for all $a,b,c\in A$. If $A$ contains 
an element $1_A\in A$ such that $1_Aa=a1_A=a$ for all $a\in A$, then $A$ is 
an unitary algebra. 

Our algebras will be finite-dimensional. 
Clearly, $\C$ is an algebra. Other 
examples of algebras are $\C[X]$ and $M_n(\C)$. 

\index{Module}
\index{Submodule}
A (left) \textbf{module} $M$ (over a unitary 
algebra $A$) is an abelian group $M$
together with a map $A\times M\to M$, $(a,m)\mapsto am$, such that
$1_Am=m$ for all $m\in M$ and 
$a(bm)=(ab)m$ and $a(m+m_1)=am+am_1$ for all $a,b\in A$ and $m,m_1\in M$. 
A \textbf{submodule} $N$ of $M$ is a subgroup 
$N$ such that $an\in N$ for all $a\in A$ and $n\in N$. 

\begin{exercise}
Let $A$ be a finite-dimensional algebra. If $M$ 
is a module, then $M$ is a vector space with 
$\lambda m=(\lambda 1_A)m$ for $\lambda\in\C$ and $m\in M$. Moreover, 
$M$ is finitely generated if and only if $M$ is finite-dimensional. 
\end{exercise}

\index{Module!simple}
\index{Module!semisimple}
A module $M$ is said to be \textbf{simple} if $M\ne\{0\}$ and $\{0\}$ and $M$ 
are the only submodules of $M$.	
A finite-dimensional module $M$  
is said to be \textbf{semisimple} if $M$ is a direct sum of 
finitely many simple submodules. 
Clearly, simple modules are semisimple. Moreover, any finite direct sum of semisimples is semisimple. 


\index{Algebra!semisimple}
A finite-dimensional algebra $A$ is said to be \textbf{semisimple} if
if every finitely-generated $A$-module is semisimple. 

\begin{theorem}[Artin--Wedderburn]
Let $A$ be a complex finite-dimensional semisimple algebra, say with  
$k$ isomorphism classes of simple modules. Then 
\[
A\simeq M_{n_1}(\C)\times\cdots\times M_{n_k}(\C)
\]
for some $n_1,\dots,n_k\in\Z_{>0}$.
\end{theorem}

We also basic some basic facts on the Jacobson radical
of finite-dimensional algebras. If $A$ is a finite-dimensional algebra, the \textbf{Jacobson radical} is defined as 
\[
J(A)=\bigcap\{M:M\text{ is a maximal left ideal of $A$}\}. 
\]
It turns out that $J(A)$ is an ideal of $A$. If $A$ is
unitary, then Zorn's lemma implies that there a 
maximal left ideal of $A$ and hence $J(A)\ne A$. 

An ideal $I$ of $A$ is said to be \textbf{nilpotent}
if $I^m=\{0\}$ for some $m$, that is 
$x_1\cdots x_m=0$ for all $x_1,\dots,x_m\in I$. 
One proves that the Jacboson radical of $A$ 
contains every nilpotent ideal of $A$. An important
fact is that 
\begin{align*}
A\text{ is semisimple }
&\Longleftrightarrow 
J(A)=\{0\}\\
&\Longleftrightarrow 
A\text{ has no non-zero nilpotent ideals}.
\end{align*}

\topic{Kolchin's theorem}
\label{Kolchin}

In this section it will be useful to consider 
non-unitary algebras. 

\begin{definition}
\index{Nil!element}
\index{Nilpotent!element}
\index{Nil!algebra}
    Let $A$ be an algebra (possibly without one). An element $a\in A$
    is said to be \textbf{nilpotent} if 
    $a^n=0$ for some $n\geq1$. The algebra $A$ is said to be
    \textbf{nil} if every element $a\in A$ is nilpotent. 
\end{definition}

Nilpotent elements are also called nil elements.  

\begin{example}
    Let $A=M_2(\R)$. Then $a=\begin{pmatrix}0&1\\0&0\end{pmatrix}$ is nilpotent. 
\end{example}

\begin{definition}
    \index{Nilpotent!algebra}
    An algebra $A$ is said to be \textbf{nilpotent} if there exists
    $n\geq1$ such that every product 
    $a_1a_2\cdots a_n$
    of $n$ elements of $A$ is zero. 
\end{definition}

Nilpotent algebras are trivially nil, whereas nil algebras may not be nilpotent, as each element being nilpotent does not force products of distinct elements to vanish.

\begin{exercise}
    Give an example of a nil algebra that is not nilpotent. 
\end{exercise}

Note that nil algebras cannot be unitary. 

\begin{proposition}
\label{pro:unit}
    Let $A$ be an algebra. There exists an algebra $B$ 
    with one $1_B$ and an ideal $I$ of $B$ 
    such that $B/I\simeq\C$ and $I\simeq A$. 
\end{proposition}

\begin{proof}[Sketch of the proof]
    Let $B=\C\times A$. The multiplication  
    \[
    (\lambda,u)(\mu,v)=(\lambda\mu,\lambda v+\mu u+uv)
    \]
    turns $B$ into an algebra with identity $(1,0)$. The subset
    $I=\{(0,a):a\in A\}$ is an ideal of $B$. Then $I\simeq A$ 
    and $B/I\simeq\C$. 
\end{proof}

\begin{exercise}
Let $A_1,\dots,A_k$ be algebras. 
Prove that the ideals of $A_1\times\cdots\times A_k$ 
are of the form $I_1\times\cdots\times I_k$, where
each $I_j$ is an ideal of $A_j$.  
\end{exercise}

\begin{exercise}
\label{xca:unit}
    Prove that the non-zero ideals of 
    $\prod_{i=1}^k M_{n_i}(\C)$ are unitary algebras.  
\end{exercise}

\begin{proposition}
    Let $A$ be non-zero algebra (possibly without one). If $A$ 
    does not have non-zero nilpotent 
    ideals, then $A$ is a unitary algebra. 
\end{proposition}

\begin{proof}
    Let $B$ be a unitary algebra such that there exists
    an ideal $I$ of $B$ with $B/I\simeq\C$ and $I\simeq A$ 
    (see Proposition \ref{pro:unit}). Let $J$ be 
    a nilpotent ideal of $B$. Since $J\cap I\subseteq I$ is a nilpotent
    ideal of $A$, 
    $J\cap I=\{0\}$. Thus 
    \[
    J\simeq J/(J\cap I)\simeq (I+J)/I
    \]
    is a nilpotent ideal of $B/I\simeq\C$. Thus $J=\{0\}$ 
    and hence $B$ is semisimple. By Artin--Wedderburn, 
    $B\simeq\prod_{i=1}^k M_{n_i}(\C)$. Since $A$ is isomorphic to an ideal of 
    $B$, Exercise \ref{xca:unit} shows
    that $A$ is a unitary algebra. 
\end{proof}

Now we prove another nice result of Wedderburn:

\begin{theorem}[Wedderburn]
\label{thm:Wedderburn}
\index{Wedderburn's theorem}
    Let $A$ be a complex finite-dimensional 
    algebra. If $A$ is generated (as a vector space) 
    by nilpotent elements, then $A$ is nilpotent. 
\end{theorem}

We shall need a lemma.

\begin{lemma}
    The vector space $M_n(\C)$ does not have a basis of nilpotent matrices. 
\end{lemma}

\begin{proof}
    If $\{A_1,\dots,A_{n^2}\}$ is a basis of 
    $M_n(\C)$ consisting of nilpotent matrices, 
    then there exist $\lambda_1,\dots,\lambda_{n^2}\in\C$ such that 
    \begin{equation}
        \label{eq:nilpotent}
        E_{11}=\begin{pmatrix}
        1&0&\cdots&0\\
        0&0&\cdots&0\\
        \vdots&\vdots&\ddots&\vdots\\
        0&0&\cdots&0
        \end{pmatrix}
        =\sum_{i=1}^{n^2}\lambda_iA_i.
    \end{equation}
    Note $\trace(A_i)=0$ for all $i\in\{1,\dots,n\}$, as 
    every $A_i$ is nilpotent. 
    Apply trace to \eqref{eq:nilpotent} to 
    obtain that $1=\trace(E_{11})=\sum\lambda_i\trace(A_i)=0$, a contradiction. 
\end{proof}

Now we prove Wedderburn's theorem. We note that
the theorem can be extended to any algebraically closed field. We 
state and proof Wedderburn's theorem in the case of complex numbers
to simplify a little bit the presentation. 

\begin{proof}[Proof of Theorem \ref{thm:Wedderburn}]
    We proceed by induction on $\dim A$. If $\dim A=1$ and 
    there exists a nilpotent element $a\in A$ such that 
    $\{a\}$ is a basis of $A$, then $A$ is nilpotent, 
    as every element of $A$ is nilpotent, as it is 
    of the form 
    $\lambda a$ for some $\lambda\in\C$. 
    
    Assume now that $\dim A>1$. Since $J(A)$ is nilpotent, $J(A)^n=\{0\}$ 
    for some $n$. 
    
    If $J(A)=A$, the result trivially holds. 
    
    If $J(A)\ne\{0\}$, 
    $\dim A/J(A)<\dim A$ and hence 
    $A/J(A)$ is nilpotent by 
    the inductive hypothesis, 
    say $(A/J(A))^m=\{0\}$. Let $\pi\colon A\to A/J(A)$ be the canonical map and 
    $N=nm$. We claim that $A^N=\{0\}$. Let $a_1,\dots,a_N\in A$. Write
    $a_1\cdots a_N=x_1\cdots x_n$ for some $x_1\cdots x_n\in A$. For example,
    \begin{align*}
    x_1&=a_1a_2\cdots a_m,\\
    x_2&=a_{m+1}a_{m+2}\cdots a_{2m},\\
    &\vdots
    \end{align*}
    Since 
    \[
    \pi(x_1)=\pi(a_1a_2\cdots a_m)=\pi(a_1)\pi(a_2)\cdots\pi(a_m)=0,
    \]
    it follows that $x_1\in J(A)$. Similarly, 
    $\pi(x_j)\in J(A)$
    for every $j\in\{1,\dots,n\}$. Thus 
    \[
    a_1a_2\cdots a_N=x_1x_2\cdots x_n\in J(A)^n=\{0\}. 
    \]
    Thus $A$ is nilpotent. 
    
    If $J(A)=\{0\}$, then 
    $A$ is semisimple. By Artin--Wedderburn, 
    $A\simeq\prod_{i=1}^k M_{n_i}(\C)$, a contradiction to 
    the previous lemma. 
\end{proof}

\begin{definition}
\index{Flag!complete}
    Let $V=\C^{n}$ (column vectors). A \textbf{complete flag} in $V$ 
    is a sequence $(V_1,V_2,\dots,V_n)$ of vector spaces
    such that 
    \[
    \{0\}\subsetneq V_1\subsetneq V_2\subsetneq\cdots\subsetneq V_n=V.
    \]
\end{definition}

\index{Flag!standard}
If $(V_1,\dots,V_n)$ is a complete flag, then $\dim V_i=i$ for all 
$i\in\{1,\dots,n\}$. 
Let $\{e_1,\dots,e_n\}$ be the standard basis of $\C^n$. 
The \textbf{standard flag} is the sequence $(E_1,\dots,E_n)$, where
$E_i=\langle e_1,\dots,e_i\rangle$ for all $i\in\{1,\dots,n\}$.  

Note that $\GL_n(\C)$ acts on the set of complete flags of $V$ 
by 
\[
g\cdot (V_1,\dots,V_n)=(T_g(V_1),\dots,T_g(V_n)),
\]
where $T_g\colon V\to V$, $x\mapsto gx$. 

The action is \emph{transitive}, 
which means that if $(V_1,\dots,V_n)$ 
is a complete flag, then there exists 
$g\in\GL_n(\C)$ such that $g\cdot (E_1,\dots,E_n)=(V_1,\dots,V_n)$. 
In fact, 
the matrix $g=(v_1|v_2|\cdots|v_n)$, where
$\{v_1,\dots,v_n\}$ is a basis of $V$, 
satisfies $ge_i=v_i$ for all $i\in\{1,\dots,n\}$. 

\label{Borel subgroup}
Let $B_n(\C)$ be the stabilizer    
\begin{align*}
G_{(E_1,\dots,E_n)}
&=\{g\in\GL_n(\C):T_g(E_i)=E_i\text{ for all $i$}\}
=\{(b_{ij}):b_{ij}=0\text{ if $i>j$}\}
\end{align*}
of the standard flag. Then $B_n(\C)$ is 
known as the \textbf{Borel subgroup}. 

Let $U_n(\C)$ be the subgroup of $\GL_n(\C)$ 
of matrices $(u_{ij})$ such that 
\[
u_{ij}=\begin{cases}
1&\text{if $i=j$},\\
0&\text{if $i>j$}.\end{cases}
\]
Let $T_n(\C)$ be the subgroup of $\GL_n(\C)$ diagonal matrices. 

\begin{proposition}
    $B_n(\C)=U_n(\C)\rtimes T_n(\C)$. 
\end{proposition}

\begin{proof}
    It is trivial that $U_n(\C)\cap T_n(\C)=\{I\}$, where $I$ is the 
    $n\times n$ identity matrix. Clearly, $U_n(\C)$ is a subgroup of $B_n(\C)$.
    To prove that 
    $U_n(\C)$ is normal in $B_n(\C)$ note that $U_n(\C)$ is the kernel
    of the group homomorphism
    \[
    f\colon B_n(\C)\to T_n(\C),\quad
    (b_{ij})\mapsto\begin{pmatrix}
        b_{11}\\
        &b_{22}\\
        &&\ddots\\
        &&&b_{nn}
    \end{pmatrix}.
    \]
    It remains to show that $B_n(\C)=U_n(\C)T_n(\C)$.
    Let us prove that  $B_n(\C)\subseteq U_n(\C)T_n(\C)$, as the other inclusion is trivial. 
    Let $b\in B_n(\C)$. Then
    $bf(b)^{-1}\in \ker f=U_n(\C)$ and therefore  
    $b=(bf(b)^{-1})f(b)\in U_n(\C)T_n(\C)$. 
\end{proof}

\begin{definition}
\index{Unipotent element}
    A matrix $a\in\GL_n(\C)$ is said to be \textbf{unipotent} 
    if its characteristic polynomial is of the form 
    $(X-1)^n$. 
\end{definition}

The matrix $\begin{pmatrix}1&1\\0&1\end{pmatrix}$ is unipotent, 
as its characteristic polynomial is $(X-1)^2$. 

\begin{definition}
\index{Unipotent group}
    A subgroup $G$ of $\GL_n(\C)$ is said to be \textbf{unipotent} if
    each $g\in G$ is unipotent. 
\end{definition}

Now an application of Wedderburn's theorem:

\begin{proposition}
\label{pro:unipotent}
    Let $G$ be an unipotent subgroup of $\GL_n(\C)$. 
    Then there exists a non-zero 
    $v\in C^{n}$ such that $gv=v$ for all $g\in G$. 
\end{proposition}

\begin{proof}
    Without loss of generality, we may assume that $G$ is non-trivial. 
    Let $V$ be the subspace of $\C^{n\times n}$ 
    generated by $\{g-I:g\in G\}$. If $g\in G$, then 
    $(g-I)^n=0$, as $g$ is unipotent. Thus
    every element of $V$ is nilpotent. If $g,h\in G$, 
    then 
    \[
    (g-I)(h-I)=(gh-I)-(g-I)-(h-I)\in V.
    \]
    This means that $V$ is closed under multiplication and
    hence $V$ is an algebra generated (as a vector space)
    by nilpotent elements. By Wedderburn's theorem, 
    $V$ is nilpotent. Let $m$ be minimal 
    such that 
    $(g_1-I)\cdots (g_m-I)=0$ 
    for all $g_1,\dots,g_m\in G$. The minimality of $m$ implies that  
    there exist $h_1,\dots,h_{m-1}\in G$ such that 
    \[
    (h_1-I)\cdots (h_{m-1}-I)\ne 0.
    \]
    In particular, there exists a non-zero 
    $w\in C^{n}$ such that 
    $v=(h_1-I)\cdots (h_{m-1}-I)w\ne 0$. For every 
    $g\in G$, 
    \[
    (g-I)v=(g-I)(h_1-I)\cdots (h_{m-1}-I)w=0
    \]
    and hence $gv=v$. 
\end{proof}

\begin{theorem}[Kolchin]
\label{thm:Kolchin}
\index{Kolchin's theorem}
Every unipotent subgroup of $\GL_n(\C)$ is conjugate
to some subgroup of $U_n(\C)$. 
\end{theorem}

\begin{proof}
    Let $G$ be an unipotent subgroup of $\GL_n(\C)$. 
    Assume first that there exists
    a complete flag $(V_1,\dots,V_n)$ of $\C^n$
    such that $G\subseteq G_{(V_1,\dots,V_n)}$. Let $g\in\GL_n(\C)$ be such that 
    $g\cdot (E_1,\dots,E_n)=(V_1,\dots,V_n)$. Then 
    \begin{gather*}
        G\subseteq G_{g\cdot (E_1,\dots,E_n)}
        =g G_{(E_1,\dots,E_n)}g^{-1}=gB_n(\C)g^{-1}.
    \shortintertext{Since $G$ is unipotent,}
        G=G\cap (gB_n(\C)g^{-1})\subseteq gU_n(\C)g^{-1}.
    \end{gather*}
    
    We claim that $G\subseteq G_{(V_1,\dots,V_n)}$ for
    some complete flag $(V_1,\dots,V_n)$. We proceed by induction on $n$. If $n=1$, the result is trivial. Assume the result holds for 
    $n-1$. By the previous proposition, there exists a non-zero $v\in\C^n$ 
    such that $gv=v$ for all $g\in G$. Let $Q=\C^n/\langle v\rangle$ and $\pi\colon \C^n\to Q$ be the canonical map. Then $\dim Q=n-1$. The group $G$ 
    acts on $Q$ by
    \[
    g\cdot (w+\langle v\rangle)=gw+\langle v\rangle.
    \]
    The action is well-defined: if $w+\langle v\rangle=w_1+\langle v\rangle$, then 
    $w-w_1=\lambda v$ for some $\lambda\in\C$. This implies
    that 
    \[
    gw-gw_1=g(w-w_1)=\lambda(gv)=\lambda v\in \langle v\rangle
    \]
    and hence $gw+\langle v\rangle=gw_1+\langle v\rangle$. 
    
    By the inductive hypothesis, $G$ stabilizes
    a complete flag $(Q_1,\dots,Q_{n-1})$, where
    \[
    Q_1=\langle\pi(v_1)\rangle,
    \quad
    Q_2=\langle\pi(v_1),\pi(v_2)\rangle,
    \quad
    \dots
    \quad
    Q_{n-1}=\langle\pi(v_1),\dots,\pi(v_{n-1})\rangle.
    \]
    Let 
    \[
    W_0=\langle v\rangle,
    \quad
    W_1=\langle v,v_1\rangle,
    \quad
    W_2=\langle v,v_1,v_2\rangle,
    \quad\dots\quad 
    W_{n-1}=\langle v,v_1,\dots,v_{n-1}\rangle.
    \]
    Since $(Q_1,\dots,Q_{n-1})$ is a complete flag, 
    the set $\{\pi(v_j):1\leq j\leq n-1\}$ is linearly
    independent. We claim that 
    $\{v,v_1,\dots,v_{n-1}\}$ is linearly independent. In fact, since $v\ne 0$, one obtains that 
    \[
    \sum_{i=1}^{n-1}\lambda_iv_i+\lambda v=0
    \implies
    \sum_{i=1}^{n-1}\lambda_i\pi(v_i)=0
    \implies 
    \lambda_1=\cdots=\lambda_{n-1}=0
    \implies
    \lambda=0.
    \]
    Thus $\dim W_i=i+1$ for all $i$. 
    
    Let $g\in G$. 
    Clearly, 
    $gW_0\subseteq W_0$, as $gv=v$. Let $j\in\{1,\dots,n-1\}$.
    There exist $\lambda_1,\dots,\lambda_j\in\C$ 
    such that 
    $\pi(gv_j)=\sum_{i\leq j}\lambda_i\pi(v_i)$. This means
    that 
    \[
    gv_j-\sum_{i\leq j}\lambda_iv_i=\lambda v\in\langle v\rangle
    \]
    for some $\lambda\in\C$. In particular, 
    \[
    gv_j=\sum_{i\leq j}\lambda_iv_i+\lambda v\in\langle v,v_1,\dots,v_{j}\rangle=W_j.
    \]
    Therefore $G\subseteq G_{(W_0,\dots,W_{n-1})}$. 
\end{proof}

\subsection{Some comments}

The ideas behind the theorem are somewhat connected to Sylow's theory. The key is to consider an explicit version of Sylow's theorem for the group $\GL_n(p)$ of invertible matrices 
with coefficients in the field $\F_p$ with $p$ elements. 

A group $G$ acts linearly on a vector space $V$ 
if $g\cdot (v+w)=g\cdot v+g\cdot w$
for all $g\in G$ and $v,w\in V$.
Proposition \ref{pro:unipotent} has the following
version:

\begin{proposition}
Let $P$ be a finite $p$-group acting on a finite-dimensional $\F_p$-vector space 
$V$ linearly. Then there exists a non-zero $v\in V$ 
such that $x\cdot v=v$ for all $x\in P$. 
\end{proposition}

\begin{proof}
    Let $n=\dim V$. There are $p^n-1$ non-zero vectors 
    in $V$. Since the action is linear, 
    $P$ acts  
    on $X=V\setminus\{0\}$. We decompose $V$ into orbits
    and collect those orbits with only one element, say 
    \[
    X=X_0\cup O(v_1)\cup \cdots\cup O(v_m),
    \]
    where $|O(v_j)|\geq 2$ for all $j\in\{1,\dots,m\}$. 
    Since 
    $p$ divides the order of each $O(v_j)$ and 
    $|X|=p^n-1$ is not divisible by $p$, 
    it follows that $X_0\ne\emptyset$. In particular, 
    there exists $v\in V$ such that $x\cdot v=v$ for
    all $x\in G$. 
\end{proof}

The analog of Kolchin's theorem is the following result:

\begin{proposition}
\label{pro:Kolchin}
    Every $p$-subgroup of $\GL_n(p)$ is conjugate to a subgroup
    of the unipotent subgroup $U_n(p)$. 
\end{proposition}

\begin{proof}[Sketch of the proof]
    Let $P$ be a $p$-subgroup of $\GL_n(p)$. 
    Then $P$ acts linearly on an $n$-dimensional 
    $\F_p$-vector space $V$ by left multiplication. 
    The previous
    proposition implies that there exists a non-zero
    $v_1\in V$
    such that $xv_1=v_1$ for all $x\in P$. Let 
    $V_1=\langle v_1\rangle$. The group $P$ 
    acts on the $(n-1)$-dimensional vector space 
    $V/V_1$ by 
    \[
    x\cdot (v+V_1)=xv+V_1.
    \]
    This action is well-defined. 
    As before, there exists a non-zero 
    vector of $V/V_1$ fixed by $P$. Thus 
    there exists $v_2\in V\setminus V_1$ such that 
    $xv_2+V_1=v_2+V_1$. Note that $\{v_1,v_2\}$ is linearly
    independent, as applying the canonical
    map $V\to V/V_1$ to 
    $\alpha v_1+\beta v_2=0$ one obtains
    that $\beta=0$ and therefore $\alpha=0$. This process
    produces a basis $\{v_1,\dots,v_n\}$ 
    of $V$ and a sequence $\{0\}\subsetneq V_1\subsetneq V_2\subsetneq\cdots\subsetneq V_n=V$, where 
    $V_j=\langle v_1,\dots,v_j\rangle$ for all $j\in\{1,\dots,n\}$. Moreover,  
    $PV_j\subseteq V_j$  and 
    $Pv_j=v_j+V_{j-1}$ for all $j$. This
    means precisely that in the basis 
    $\{v_1,\dots,v_n\}$ 
    every element of $P$ is an upper triangular
    matrix with ones in the main diagonal. 
\end{proof}

\index{Sylow's theorems}
Proposition \ref{pro:Kolchin} is deeply 
connected to Sylow's theorems. 

\begin{exercise}
    Prove that the normalizer of $U_n(p)$ in $\GL_n(p)$ is the
    Borel subgroup $B_n(p)$ of upper triangular matrices. 
\end{exercise}

Now we have the following explicit Sylow theory for
$\GL_n(p)$. The first two Sylow theorems 
appear in the following result. 

\begin{exercise}
    Prove that  $U_n(p)$ is a Sylow $p$-subgroup of $\GL_n(p)$. 
\end{exercise}

What about the third Sylow's theorem? 
First note that the number $n_p$
of conjugates of $U_n(p)$ in $\GL_n(p)$ 
is the number of complete flags 
in $\F_p^n$.

\begin{exercise}
    Prove that $n_p\equiv1\bmod p$. 
\end{exercise}

